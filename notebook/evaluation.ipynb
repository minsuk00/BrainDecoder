{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/choi/BrainDecoder/outputs/samplelevel2img-samples/samples'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = \"/home/choi/BrainDecoder/\"\n",
    "dir_path = os.path.join(root_path, \"outputs\", \"samplelevel2img-samples\", \"samples\")\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gaussian37.github.io/vision-concept-ssim/\n",
    "# https://scikit-image.org/docs/dev/api/skimage.metrics.html#skimage.metrics.structural_similarity\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11288692965251086 0.07803935657510917 0.09784093551992962\n"
     ]
    }
   ],
   "source": [
    "origin_name = \"0069_00353\"\n",
    "img_name1 = \"0069_00354\"\n",
    "img_name2 = \"0069_00355\"\n",
    "img_name3 = \"0069_00356\"\n",
    "\n",
    "\n",
    "def get_img_path(filename):\n",
    "    if not filename.endswith(\".png\"):\n",
    "        filename += \".png\"\n",
    "    return os.path.join(dir_path, filename)\n",
    "\n",
    "\n",
    "origin = cv2.cvtColor(cv2.imread(get_img_path(origin_name)), cv2.COLOR_BGR2RGB)\n",
    "img1 = cv2.cvtColor(cv2.imread(get_img_path(img_name1)), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread(get_img_path(img_name2)), cv2.COLOR_BGR2RGB)\n",
    "img3 = cv2.cvtColor(cv2.imread(get_img_path(img_name3)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "ssim_1 = ssim(origin, img1, channel_axis=2)\n",
    "# ssim_1, diff1 = ssim(origin, img1, channel_axis=2, full=True)\n",
    "# diff1 = (diff1 * 255).astype(\"uint8\")\n",
    "ssim_2 = ssim(origin, img2, channel_axis=2)\n",
    "# ssim_2, diff2 = ssim(origin, img2, channel_axis=2, full=True)\n",
    "# diff2 = (diff2 * 255).astype(\"uint8\")\n",
    "ssim_3 = ssim(origin, img3, channel_axis=2)\n",
    "# ssim_3, diff3 = ssim(origin, img3, channel_axis=2, full=True)\n",
    "# diff3 = (diff3 * 255).astype(\"uint8\")\n",
    "\n",
    "print(ssim_1, ssim_2, ssim_3)\n",
    "\n",
    "# 로컬 영역에서 SSIM 구하기\n",
    "# ssim_1, diff1 = ssim(origin, img1, channel_axis=2, win_size=11, full=True)\n",
    "# diff1 = (diff1 * 255).astype(\"uint8\")\n",
    "\n",
    "# 이미지간 차이 시각화하기\n",
    "# fig, ax = plt.subplots(ncols=3)\n",
    "# ax[0].imshow(diff1)\n",
    "# ax[1].imshow(diff2)\n",
    "# ax[2].imshow(diff3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "# _ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img1 = \"0069_00354\"\n",
    "gen_img2 = \"0069_00355\"\n",
    "gen_img3 = \"0069_00356\"\n",
    "\n",
    "def get_img_path(filename):\n",
    "    if not filename.endswith(\".png\"):\n",
    "        filename += \".png\"\n",
    "    return os.path.join(dir_path, filename)\n",
    "\n",
    "def get_img_tensor(filename):\n",
    "    img = Image.open(get_img_path(filename))\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    return convert_tensor(img).type(dtype=torch.uint8)\n",
    "\n",
    "# gen_imgs = [gen_img1,gen_img2,gen_img3]\n",
    "# gen_imgs = torch.stack([get_img_tensor(filename) for filename in gen_imgs])\n",
    "gen_imgs = torch.stack([get_img_tensor(filename) for filename in os.listdir(dir_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/braindecoder/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.0175), tensor(0.0117))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = InceptionScore()\n",
    "# generate some images\n",
    "# imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "inception.update(gen_imgs)\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "# _ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_img1 = \"0069_00353\"\n",
    "gt_img2 = \"0069_00353\"\n",
    "gt_img3 = \"0069_00353\"\n",
    "gen_img1 = \"0069_00354\"\n",
    "gen_img2 = \"0069_00355\"\n",
    "gen_img3 = \"0069_00356\"\n",
    "\n",
    "def get_img_path(filename):\n",
    "    if not filename.endswith(\".png\"):\n",
    "        filename += \".png\"\n",
    "    return os.path.join(dir_path, filename)\n",
    "\n",
    "def get_img_tensor(filename):\n",
    "    img = Image.open(get_img_path(filename))\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    return convert_tensor(img).type(dtype=torch.uint8)\n",
    "\n",
    "gt_imgs = [gt_img1,gt_img2,gt_img3]\n",
    "gt_imgs = torch.stack([get_img_tensor(filename) for filename in gt_imgs])\n",
    "\n",
    "gen_imgs = [gen_img1,gen_img2,gen_img3]\n",
    "gen_imgs = torch.stack([get_img_tensor(filename) for filename in gen_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/braindecoder/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `FrechetInceptionDistance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/choi/Downloads/miniconda3/envs/braindecoder/lib/python3.11/site-packages/scipy/linalg/_matfuncs_sqrtm.py:205: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  arg2 = norm(X.dot(X) - A, 'fro')**2 / norm(A, 'fro')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.7531e-05)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature = one of [64, 192, 768, 2048]\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "# generate two slightly overlapping image intensity distributions\n",
    "# imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "# imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "fid.update(gt_imgs, real=True)\n",
    "fid.update(gen_imgs, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-way Top-k Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
    "import torch\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def n_way_top_k_acc(pred, class_id, n_way, num_trials=40, top_k=1):\n",
    "    pick_range = [i for i in np.arange(len(pred)) if i != class_id]\n",
    "    acc_list = []\n",
    "    for t in range(num_trials):\n",
    "        idxs_picked = np.random.choice(pick_range, n_way - 1, replace=False)\n",
    "        pred_picked = torch.cat([pred[class_id].unsqueeze(0), pred[idxs_picked]])\n",
    "        acc = accuracy(\n",
    "            pred_picked.unsqueeze(0), torch.tensor([0], device=pred.device), top_k=top_k\n",
    "        )\n",
    "        acc_list.append(acc.item())\n",
    "    return np.mean(acc_list), np.std(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_n_way_top_k_acc(\n",
    "    pred_imgs, ground_truth, n_way, num_trials, top_k, device, return_std=False\n",
    "):\n",
    "    weights = ViT_H_14_Weights.DEFAULT\n",
    "    model = vit_h_14(weights=weights)\n",
    "    preprocess = weights.transforms()\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    acc_list = []\n",
    "    std_list = []\n",
    "    for pred, gt in zip(pred_imgs, ground_truth):\n",
    "        pred = (\n",
    "            preprocess(Image.fromarray(pred.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        )\n",
    "        gt = preprocess(Image.fromarray(gt.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        gt_class_id = model(gt).squeeze(0).softmax(0).argmax().item()\n",
    "        pred_out = model(pred).squeeze(0).softmax(0).detach()\n",
    "        # print(pred_out.shape)\n",
    "        # print(pred_out)\n",
    "        # print(gt_class_id)\n",
    "        # print(pred_out[gt_class_id])\n",
    "\n",
    "        acc, std = n_way_top_k_acc(pred_out, gt_class_id, n_way, num_trials, top_k)\n",
    "        acc_list.append(acc)\n",
    "        std_list.append(std)\n",
    "\n",
    "    if return_std:\n",
    "        return acc_list, std_list\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "origin_name = \"0069_00353\"\n",
    "img_name1 = \"0069_00354\"\n",
    "img_name2 = \"0069_00355\"\n",
    "img_name3 = \"0069_00356\"\n",
    "img_name4 = \"0069_00349\"\n",
    "\n",
    "def get_img_path(filename):\n",
    "    if not filename.endswith(\".png\"):\n",
    "        filename += \".png\"\n",
    "    return os.path.join(dir_path, filename)\n",
    "\n",
    "gt = Image.open(get_img_path(origin_name))\n",
    "# gt = [np.array(gt),np.array(gt),np.array(gt)]\n",
    "# display(gt)\n",
    "gt = [np.array(gt)]\n",
    "\n",
    "# canoe\n",
    "pred1 = Image.open(get_img_path(img_name1))\n",
    "pred2 = Image.open(get_img_path(img_name2))\n",
    "pred3 = Image.open(get_img_path(img_name3))\n",
    "# piano\n",
    "pred4 = Image.open(get_img_path(img_name4))\n",
    "# display(pred4)\n",
    "# pred = [np.array(pred1),np.array(pred2),np.array(pred3)]\n",
    "pred = [np.array(pred4)]\n",
    "\n",
    "get_n_way_top_k_acc(\n",
    "            pred,\n",
    "            gt,\n",
    "            n_way=50,\n",
    "            num_trials=1000,\n",
    "            top_k=1,\n",
    "            device=\"cuda\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braindecoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
