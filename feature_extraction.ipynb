{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lookup_dict import lookup_dict, id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"\"\n",
    "dataset_path = os.path.join(root_path, \"dataset\")\n",
    "images_dataset_path = os.path.join(dataset_path, \"imageNet_images\")\n",
    "eeg_dataset_path = os.path.join(dataset_path, \"eeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset_1 (using custom dataset method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all image folders to create list of images\n",
    "\n",
    "import random\n",
    "\n",
    "dir_list = list(os.walk(images_dataset_path))\n",
    "# skip any unnecessary folders\n",
    "start_idx = len(dir_list) - 40\n",
    "images_list = []\n",
    "for sub_dir in dir_list[start_idx:]:\n",
    "    # images_list+=sub_dir[2]\n",
    "    images_list.extend(sub_dir[2])\n",
    "\n",
    "images_list = [image_name.replace(\".JPEG\", \"\") for image_name in images_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split images list to 8:1:1\n",
    "\n",
    "# random.shuffle(images_list)\n",
    "\n",
    "images_total_size = len(images_list)\n",
    "train_size = int(images_total_size * 0.8)\n",
    "val_size = int(images_total_size * 0.1)\n",
    "test_size = images_total_size - train_size - val_size\n",
    "\n",
    "train_images = images_list[:train_size]\n",
    "val_images = images_list[train_size : train_size + val_size]\n",
    "test_images = images_list[-1 * test_size :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_dataset_name = \"eeg_5_95_std.pth\"\n",
    "eeg_dataset = torch.load(os.path.join(eeg_dataset_path, eeg_dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_list) -> None:\n",
    "        super().__init__()\n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        # self.y_data = [image_name.split(\"_\")[0] for image_name in images_list]\n",
    "\n",
    "        for eeg_segment in eeg_dataset[\"dataset\"]:\n",
    "            for image_name in images_list:\n",
    "                if eeg_dataset[\"images\"][eeg_segment[\"image\"]] == image_name:\n",
    "                    self.x_data.append(eeg_segment[\"eeg\"][:, 20:460])\n",
    "                    # all_channel_list = np.array(eeg_segment['eeg'])\n",
    "                    # self.x_data.append(torch.from_numpy(all_channel_list[:,40:480]))\n",
    "                    # self.x_data.append(torch.FloatTensor([eeg_sequence[40:480] for eeg_sequence in eeg_segment['eeg']]))\n",
    "                    class_id = image_name.split(\"_\")[0]\n",
    "                    self.y_data.append(lookup_dict[class_id])\n",
    "                    # self.y_data.append(eeg_dataset['labels'][eeg_segment['label']])\n",
    "                    break\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_images)\n",
    "val_dataset = CustomDataset(val_images)\n",
    "test_dataset = CustomDataset(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset_2 (using splitter method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, eeg_dataset_file_name=\"eeg_5_95_std.pth\") -> None:\n",
    "        super().__init__()\n",
    "        loaded = torch.load(os.path.join(eeg_dataset_path, eeg_dataset_file_name))\n",
    "        self.data = loaded[\"dataset\"]\n",
    "        self.labels = loaded[\"labels\"]\n",
    "        self.images = loaded[\"images\"]\n",
    "        self.size = len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # t() -> transpose\n",
    "        eeg = self.data[idx][\"eeg\"].t()\n",
    "        eeg = eeg[20:460, :]\n",
    "\n",
    "        label = self.data[idx][\"label\"]\n",
    "        return eeg, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(Dataset):\n",
    "    def __init__(self, dataset, split_name=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "        loaded = torch.load(\n",
    "            os.path.join(eeg_dataset_path, \"block_splits_by_image_all.pth\")\n",
    "        )\n",
    "        self.target_data_indices = loaded[\"splits\"][0][split_name]\n",
    "        # filter data that is too short\n",
    "        self.target_data_indices = [\n",
    "            i\n",
    "            for i in self.target_data_indices\n",
    "            if 450 <= self.dataset.data[i][\"eeg\"].size(1) <= 600\n",
    "        ]\n",
    "\n",
    "        self.size = len(self.target_data_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg, label = self.dataset[self.target_data_indices[idx]]\n",
    "        return eeg, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(eeg_dataset_file_name=\"eeg_5_95_std.pth\")\n",
    "loaders = {\n",
    "    split: DataLoader(\n",
    "        Splitter(dataset, split_name=split), batch_size=16, shuffle=True, drop_last=True\n",
    "    )\n",
    "    for split in [\"train\", \"val\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "seed = 1563423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with classifier attached\n",
    "class FeatureExtractorNN(L.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # seed_everything(seed,workers=True)\n",
    "\n",
    "        self.input_size = 128\n",
    "        self.hidden_size = 128\n",
    "        self.lstm_layers = 1\n",
    "        self.out_size = 128\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=128,hidden_size=128,num_layers=128)\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size,\n",
    "            self.hidden_size,\n",
    "            num_layers=self.lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.out_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Linear(in_features=self.out_size, out_features=40),\n",
    "            # don't use softmax with cross entropy loss\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        # self.loss_fn = nn.NLLLoss()\n",
    "        self.training_step_outputs = {\"correct_num\": 0, \"loss_sum\": 0}\n",
    "        self.validation_step_outputs = {\"correct_num\": 0, \"loss_sum\": 0}\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        lstm_init = (\n",
    "            torch.zeros(self.lstm_layers, batch_size, self.hidden_size),\n",
    "            torch.zeros(self.lstm_layers, batch_size, self.hidden_size),\n",
    "        )\n",
    "        lstm_init = (lstm_init[0].to(device), lstm_init[0].to(device))\n",
    "\n",
    "        # dont need to transpose because already transposed when creating dataset\n",
    "        # input = input.transpose(1,2)\n",
    "\n",
    "        lstm_out, _ = self.lstm(input, lstm_init)\n",
    "        # tmp_out = lstm_out[:,-1,:] if input.dim()==3 else lstm_out[-1,:]\n",
    "        tmp_out = lstm_out[:, -1, :]\n",
    "        out = self.output(tmp_out)\n",
    "        # print(\"out shape\",out.shape)\n",
    "        res = self.classifer(out)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return optim.SGD(self.parameters(),lr=1e-4,weight_decay=0.1)\n",
    "        # return optim.Adam(self.parameters(),lr=1e-3,weight_decay=0.1)\n",
    "        # optimizer = optim.Adam(self.parameters(), lr=(1e-4) * 4, weight_decay=0.001)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=(1e-3))\n",
    "        # optimizer = optim.SGD(self.parameters(), lr=(1e-4) * 5)\n",
    "        # self.scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        #     optimizer, lambda epoch: 0.95**epoch\n",
    "        # )\n",
    "        # self.scheduler = optim.lr_scheduler.CyclicLR(optimizer,base_lr=0.0001, max_lr=0.01,step_size_up=5,mode=\"triangular2\")\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        return [optimizer], [self.scheduler]\n",
    "        # return [optimizer]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "\n",
    "        self.log_dict({\"train_loss\": loss}, prog_bar=True, on_epoch=True)\n",
    "        preds = out.argmax(dim=1)\n",
    "        self.training_step_outputs[\"correct_num\"] += (preds == y).sum()\n",
    "        self.training_step_outputs[\"loss_sum\"] += loss\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        num_correct = self.training_step_outputs[\"correct_num\"]\n",
    "        acc = num_correct / loaders[\"train\"].dataset.__len__()\n",
    "        loss = self.training_step_outputs[\"loss_sum\"] / loaders[\"train\"].__len__()\n",
    "        print(\"\\n\")\n",
    "        # print(\"EPOCH:\",self.current_epoch)\n",
    "        print(\n",
    "            f\"Training accuracy: {acc.item()} ({num_correct.item()}/{loaders['train'].dataset.__len__()} correct)\"\n",
    "        )\n",
    "        print(\"Training loss (average):\", loss.item())\n",
    "        print(\"\\n\")\n",
    "        self.training_step_outputs[\"correct_num\"] = 0\n",
    "        self.training_step_outputs[\"loss_sum\"] = 0\n",
    "\n",
    "        print(\"Learning rate:\", self.scheduler.get_last_lr())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "\n",
    "        self.log_dict({\"val_loss\": loss}, prog_bar=True, on_epoch=True)\n",
    "        preds = out.argmax(dim=1)\n",
    "        self.validation_step_outputs[\"correct_num\"] += (preds == y).sum()\n",
    "        self.validation_step_outputs[\"loss_sum\"] += loss\n",
    "        # return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        num_correct = self.validation_step_outputs[\"correct_num\"]\n",
    "        acc = num_correct / loaders[\"val\"].dataset.__len__()\n",
    "        loss = self.validation_step_outputs[\"loss_sum\"] / loaders[\"val\"].__len__()\n",
    "        print(\"\\n\")\n",
    "        # print(\"EPOCH:\",self.current_epoch)\n",
    "        print(\n",
    "            f\"Validation accuracy: {acc.item()} ({num_correct.item()}/{loaders['val'].dataset.__len__()} correct)\"\n",
    "        )\n",
    "        print(\"Validation loss (average):\", loss.item())\n",
    "        print(\"\\n\")\n",
    "        self.validation_step_outputs[\"correct_num\"] = 0\n",
    "        self.validation_step_outputs[\"loss_sum\"] = 0\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        out = self(x)\n",
    "        loss = self.loss_fn(out, y)\n",
    "\n",
    "        y_hat = torch.argmax(out, dim=1)\n",
    "        # print(\"OUT,YHAT:\",out,y_hat)\n",
    "        test_acc = torch.sum(y == y_hat).item() / (len(y) * 1.0)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"test_loss\": loss, \"test_acc\": test_acc}, prog_bar=True, on_epoch=True\n",
    "        )\n",
    "        # print(\"   ||   test loss:\",loss.item(), \"   ||   test accuracy:\",test_acc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 124/124 [00:01<00:00, 114.79it/s]\n",
      "\n",
      "Validation accuracy: 0.017552657052874565 (35/1994 correct)\n",
      "Validation loss (average): 3.6950571537017822\n",
      "\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 124/124 [00:01<00:00, 114.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.6950571537017822     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.6950571537017822    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.6950571537017822}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed = 1563423\n",
    "model = FeatureExtractorNN()\n",
    "model.to(device)\n",
    "trainer = L.Trainer()\n",
    "# trainer.validate(model,dataloaders=val_loader)\n",
    "trainer.validate(model, dataloaders=loaders[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/ms/cs/ML/NeuroImagen/lightning_logs/Adam_1-e3_StepLR_10_0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 124/124 [00:01<00:00, 108.35it/s]\n",
      "\n",
      "Validation accuracy: 0.020561685785651207 (41/1994 correct)\n",
      "Validation loss (average): 3.6930313110351562\n",
      "\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 124/124 [00:01<00:00, 107.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.6930313110351562     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.6930313110351562    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | lstm      | LSTM             | 132 K \n",
      "1 | output    | Sequential       | 16.5 K\n",
      "2 | classifer | Sequential       | 5.2 K \n",
      "3 | loss_fn   | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "153 K     Trainable params\n",
      "0         Non-trainable params\n",
      "153 K     Total params\n",
      "0.615     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 92.73it/s]\n",
      "\n",
      "Validation accuracy: 0.001003009034320712 (2/1994 correct)\n",
      "Validation loss (average): 0.05971241369843483\n",
      "\n",
      "\n",
      "Epoch 0: 100%|██████████| 497/497 [00:15<00:00, 31.50it/s, v_num=0, train_loss_step=3.400]\n",
      "\n",
      "Validation accuracy: 0.03460381180047989 (69/1994 correct)\n",
      "Validation loss (average): 3.5838534832000732\n",
      "\n",
      "\n",
      "Epoch 0: 100%|██████████| 497/497 [00:17<00:00, 28.70it/s, v_num=0, train_loss_step=3.400, val_loss=3.580, train_loss_epoch=3.650]\n",
      "\n",
      "Training accuracy: 0.036688026040792465 (292/7959 correct)\n",
      "Training loss (average): 3.6482152938842773\n",
      "\n",
      "\n",
      "Learning rate: [0.001]\n",
      "Epoch 1: 100%|██████████| 497/497 [00:17<00:00, 28.06it/s, v_num=0, train_loss_step=3.140, val_loss=3.580, train_loss_epoch=3.650]\n",
      "\n",
      "Validation accuracy: 0.054663993418216705 (109/1994 correct)\n",
      "Validation loss (average): 3.622897148132324\n",
      "\n",
      "\n",
      "Epoch 1: 100%|██████████| 497/497 [00:19<00:00, 25.75it/s, v_num=0, train_loss_step=3.140, val_loss=3.620, train_loss_epoch=3.410]\n",
      "\n",
      "Training accuracy: 0.05892699956893921 (469/7959 correct)\n",
      "Training loss (average): 3.4115207195281982\n",
      "\n",
      "\n",
      "Learning rate: [0.001]\n",
      "Epoch 2: 100%|██████████| 497/497 [00:20<00:00, 24.74it/s, v_num=0, train_loss_step=3.470, val_loss=3.620, train_loss_epoch=3.410]\n",
      "\n",
      "Validation accuracy: 0.0732196569442749 (146/1994 correct)\n",
      "Validation loss (average): 3.184084177017212\n",
      "\n",
      "\n",
      "Epoch 2: 100%|██████████| 497/497 [00:21<00:00, 22.90it/s, v_num=0, train_loss_step=3.470, val_loss=3.180, train_loss_epoch=3.200]\n",
      "\n",
      "Training accuracy: 0.08518657833337784 (678/7959 correct)\n",
      "Training loss (average): 3.1988682746887207\n",
      "\n",
      "\n",
      "Learning rate: [0.001]\n",
      "Epoch 3:  45%|████▍     | 223/497 [00:08<00:11, 24.88it/s, v_num=0, train_loss_step=3.000, val_loss=3.180, train_loss_epoch=3.200]"
     ]
    }
   ],
   "source": [
    "version_num = 96\n",
    "epoch = 9\n",
    "step = 23920\n",
    "PATH = os.path.join(\n",
    "    root_path,\n",
    "    \"lightning_logs\",\n",
    "    \"version_\" + str(version_num),\n",
    "    \"checkpoints\",\n",
    "    \"epoch=\" + str(epoch) + \"-step=\" + str(step) + \".ckpt\",\n",
    ")\n",
    "\n",
    "model = FeatureExtractorNN()\n",
    "model.to(device)\n",
    "# model = FeatureExtractorNN.load_from_checkpoint(PATH)\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "logger = TensorBoardLogger(\n",
    "    \"/Users/ms/cs/ML/NeuroImagen/lightning_logs\",\n",
    "    name=\"Adam_1-e3_StepLR_10_0.5\",\n",
    "    version=None,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=200, callbacks=[lr_monitor], logger=logger)\n",
    "# trainer.fit(model,train_dataloaders=train_loader,val_dataloaders=val_loader,ckpt_path=PATH)\n",
    "trainer.validate(model, dataloaders=loaders[\"val\"])\n",
    "trainer.fit(model, train_dataloaders=loaders[\"train\"], val_dataloaders=loaders[\"val\"])\n",
    "# trainer.fit(model,train_dataloaders=train_loader)\n",
    "# trainer.validate(model, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 124/124 [00:01<00:00, 108.29it/s]\n",
      "\n",
      "Validation accuracy: 0.2096288800239563 (418/1994 correct)\n",
      "Validation loss (average): 3.610106945037842\n",
      "\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 124/124 [00:01<00:00, 107.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.610106945037842     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.610106945037842    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.610106945037842}]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, dataloaders=loaders[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   1%|▏         | 1/76 [00:00<00:01, 61.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 76/76 [00:00<00:00, 89.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.689608573913574     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.689608573913574    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 3.689608573913574, 'test_acc': 0.0}]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at mps:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(query.shape)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m query\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pred \u001b[39m=\u001b[39m model(query)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(pred)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(pred,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# print(\"INPUT:\",input.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# tmp_out = lstm_out[:,-1,:] if input.dim()==3 else lstm_out[-1,:]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m tmp_out \u001b[39m=\u001b[39m lstm_out[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at mps:0"
     ]
    }
   ],
   "source": [
    "idx = 6\n",
    "query = val_dataset[idx][0].unsqueeze(dim=0)\n",
    "# print(query.shape)\n",
    "query.to(device)\n",
    "pred = model(query)\n",
    "# print(pred)\n",
    "pred = torch.argmax(pred, dim=1)\n",
    "# pred = pred.max(dim=1)\n",
    "# print(pred)\n",
    "print(\"predicted: \", id_to_name[lookup_dict[pred.item()]])\n",
    "print(\"answer: \", id_to_name[lookup_dict[val_dataset[idx][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 64])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb Cell 27\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y\u001b[39m=\u001b[39my\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m out \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_hat \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(y==y_hat)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(tmp_out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# print(\"out shape\",out.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifer(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ms/cs/ML/NeuroImagen/feature_extraction.ipynb#X33sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/functional.py:2476\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2463\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2464\u001b[0m         batch_norm,\n\u001b[1;32m   2465\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m   2474\u001b[0m     )\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m-> 2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/torch/nn/functional.py:2444\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2442\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m   2443\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 64])"
     ]
    }
   ],
   "source": [
    "# Calculate test accuracy\n",
    "num_correct = 0\n",
    "model.to(device)\n",
    "for x, y in test_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    out = model(x)\n",
    "    y_hat = out.argmax(dim=1)\n",
    "    # print(y==y_hat)\n",
    "    num_correct += (y == y_hat).sum()\n",
    "acc = num_correct / len(test_loader.dataset)\n",
    "print(\"Accuracy:\", acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='mps:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroimagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
