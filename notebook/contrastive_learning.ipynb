{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/Users/ms/cs/ML/NeuroImagen/\"\n",
    "dataset_path = os.path.join(root_path, \"dataset\")\n",
    "images_dataset_path = os.path.join(dataset_path, \"imageNet_images\")\n",
    "eeg_dataset_path = os.path.join(dataset_path, \"eeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, eeg_dataset_file_name=\"eeg_5_95_std.pth\") -> None:\n",
    "        super().__init__()\n",
    "        loaded = torch.load(os.path.join(eeg_dataset_path, eeg_dataset_file_name))\n",
    "        self.data = loaded[\"dataset\"]\n",
    "        self.labels = loaded[\"labels\"]\n",
    "        self.images = loaded[\"images\"]\n",
    "        self.size = len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # t() -> transpose\n",
    "        eeg = self.data[idx][\"eeg\"].t()\n",
    "        eeg = eeg[20:460, :]\n",
    "\n",
    "        label = self.data[idx][\"label\"]\n",
    "        return eeg, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(Dataset):\n",
    "    def __init__(self, dataset, split_name=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "        loaded = torch.load(\n",
    "            os.path.join(eeg_dataset_path, \"block_splits_by_image_all.pth\")\n",
    "        )\n",
    "        self.target_data_indices = loaded[\"splits\"][0][split_name]\n",
    "        # filter data that is too short\n",
    "        self.target_data_indices = [\n",
    "            i\n",
    "            for i in self.target_data_indices\n",
    "            if 450 <= self.dataset.data[i][\"eeg\"].size(1) <= 600\n",
    "        ]\n",
    "\n",
    "        self.size = len(self.target_data_indices)\n",
    "        self.all_labels = np.array(self.get_all_labels())\n",
    "        self.all_eegs = self.get_all_eegs()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eeg, label = self.dataset[self.target_data_indices[idx]]\n",
    "        return eeg, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_all_labels(self):\n",
    "        data = [self.dataset[idx] for idx in self.target_data_indices]\n",
    "        return [item[1] for item in data]\n",
    "\n",
    "    def get_all_eegs(self):\n",
    "        data = [self.dataset[idx] for idx in self.target_data_indices]\n",
    "        return [item[0] for item in data]\n",
    "\n",
    "    def generate_data_points(self, anchor_labels, positive=True):\n",
    "        eeg_shape = self.__getitem__(0)[0].size()\n",
    "        eegs = torch.empty(0, eeg_shape[0], eeg_shape[1])\n",
    "        labels = torch.empty(0)\n",
    "        for anchor_label in anchor_labels:\n",
    "            indices = (\n",
    "                np.argwhere(self.all_labels == anchor_label.item())[:, 0]\n",
    "                if positive\n",
    "                else np.argwhere(self.all_labels != anchor_label.item())[:, 0]\n",
    "            )\n",
    "            data_idx = np.random.choice(indices)\n",
    "            eeg = self.all_eegs[data_idx]\n",
    "            eegs = torch.cat((eegs, eeg.unsqueeze(dim=0)))\n",
    "            labels = torch.cat(\n",
    "                (labels, torch.tensor(self.all_labels[data_idx]).unsqueeze(dim=0))\n",
    "            )\n",
    "\n",
    "        return eegs, labels\n",
    "\n",
    "    def get_data(self, anchor_label, positive: bool = True):\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            idx = random.choice(self.target_data_indices)\n",
    "            if positive and self.dataset[idx][1] == anchor_label:\n",
    "                return self.dataset[idx]\n",
    "            if not positive and self.dataset[idx][1] != anchor_label:\n",
    "                return self.dataset[idx]\n",
    "\n",
    "            if cnt >= 2000:\n",
    "                raise Exception(f\"get_data failed after {cnt} tries\")\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(eeg_dataset_file_name=\"eeg_5_95_std.pth\")\n",
    "loaders = {\n",
    "    split: DataLoader(\n",
    "        Splitter(dataset, split_name=split), batch_size=16, shuffle=True, drop_last=True\n",
    "    )\n",
    "    for split in [\"train\", \"val\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor_ContrastiveLearning_NN(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Triplet loss\n",
    "        def dist_fn(x1, x2):\n",
    "            return torch.sum(torch.pow(torch.subtract(x1, x2), 2), dim=0)\n",
    "\n",
    "        self.loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "            distance_function=dist_fn, margin=config[\"margin\"]\n",
    "        )\n",
    "\n",
    "        # model\n",
    "        self.input_size = 128\n",
    "        self.hidden_size = 128\n",
    "        self.lstm_layers = 1\n",
    "        self.out_size = 128\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_size,\n",
    "            self.hidden_size,\n",
    "            num_layers=self.lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.out_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.to(device)\n",
    "\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "        res = self.output(lstm_out[:, -1, :])\n",
    "        return res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        anchor_eeg, anchor_label = batch\n",
    "\n",
    "        # positive_eeg = torch.empty(\n",
    "        #     (0, anchor_eeg[0].shape[0], anchor_eeg[0].shape[1]), dtype=torch.float32\n",
    "        # )\n",
    "        # negative_eeg = torch.empty(\n",
    "        #     (0, anchor_eeg[0].shape[0], anchor_eeg[0].shape[1]), dtype=torch.float32\n",
    "        # )\n",
    "        # for label in anchor_label:\n",
    "        #     positive_eeg = torch.cat(\n",
    "        #         (\n",
    "        #             positive_eeg,\n",
    "        #             loaders[\"train\"]\n",
    "        #             .dataset.get_data(label, positive=True)[0]\n",
    "        #             .unsqueeze(dim=0),\n",
    "        #         )\n",
    "        #     )\n",
    "        #     negative_eeg = torch.cat(\n",
    "        #         (\n",
    "        #             negative_eeg,\n",
    "        #             loaders[\"train\"]\n",
    "        #             .dataset.get_data(label, positive=False)[0]\n",
    "        #             .unsqueeze(dim=0),\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        positive_eeg, positive_label = loaders[\"train\"].dataset.generate_data_points(\n",
    "            anchor_label, positive=True\n",
    "        )\n",
    "        negative_eeg, negative_label = loaders[\"train\"].dataset.generate_data_points(\n",
    "            anchor_label, positive=False\n",
    "        )\n",
    "\n",
    "        anchor_feature = self(anchor_eeg)\n",
    "        positive_feature = self(positive_eeg)\n",
    "        negative_eeg = self(negative_eeg)\n",
    "\n",
    "        loss = self.loss_fn(anchor_feature, positive_feature, negative_eeg)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"lr\": self.scheduler.get_last_lr()[0]},\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        anchor_eeg, anchor_label = batch\n",
    "\n",
    "        # positive_eeg = torch.empty(\n",
    "        #     (0, anchor_eeg[0].shape[0], anchor_eeg[0].shape[1]), dtype=torch.float32\n",
    "        # )\n",
    "        # negative_eeg = torch.empty(\n",
    "        #     (0, anchor_eeg[0].shape[0], anchor_eeg[0].shape[1]), dtype=torch.float32\n",
    "        # )\n",
    "        # for label in anchor_label:\n",
    "        #     positive_eeg = torch.cat(\n",
    "        #         (\n",
    "        #             positive_eeg,\n",
    "        #             loaders[\"val\"]\n",
    "        #             .dataset.get_data(label, positive=True)[0]\n",
    "        #             .unsqueeze(dim=0),\n",
    "        #         )\n",
    "        #     )\n",
    "        #     negative_eeg = torch.cat(\n",
    "        #         (\n",
    "        #             negative_eeg,\n",
    "        #             loaders[\"val\"]\n",
    "        #             .dataset.get_data(label, positive=False)[0]\n",
    "        #             .unsqueeze(dim=0),\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        positive_eeg, positive_label = loaders[\"val\"].dataset.generate_data_points(\n",
    "            anchor_label, positive=True\n",
    "        )\n",
    "        negative_eeg, negative_label = loaders[\"val\"].dataset.generate_data_points(\n",
    "            anchor_label, positive=False\n",
    "        )\n",
    "\n",
    "        anchor_feature = self(anchor_eeg)\n",
    "        positive_feature = self(positive_eeg)\n",
    "        negative_eeg = self(negative_eeg)\n",
    "\n",
    "        loss = self.loss_fn(anchor_feature, positive_feature, negative_eeg)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": loss},\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        if config[\"optimizer\"] == \"Adam\":\n",
    "            return optim.Adam(\n",
    "                self.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight-decay\"]\n",
    "            )\n",
    "        elif config[\"optimizer\"] == \"AdamW\":\n",
    "            return optim.AdamW(\n",
    "                self.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight-decay\"]\n",
    "            )\n",
    "        elif config[\"optimizer\"] == \"SGD\":\n",
    "            return optim.SGD(\n",
    "                self.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight-decay\"]\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"optimizer config error\")\n",
    "\n",
    "    def create_scheduler(self, optimizer):\n",
    "        if config[\"scheduler\"] == \"LambdaLR\":\n",
    "            return optim.lr_scheduler.LambdaLR(\n",
    "                optimizer, lambda epoch: config[\"lambda-factor\"] ** epoch\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"scheduler config error\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        # scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.95**epoch)\n",
    "        optimizer = self.create_optimizer()\n",
    "        scheduler = self.create_scheduler(optimizer)\n",
    "        self.scheduler = scheduler\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"optimizer\": \"Adam\",  # (\"Adam\", \"AdamW\", \"SGD\")\n",
    "    \"lr\": 1e-4,\n",
    "    \"scheduler\": \"LambdaLR\",\n",
    "    \"lambda-factor\": 0.95,\n",
    "    \"weight-decay\": 0,\n",
    "    \"margin\": 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /Users/ms/cs/ML/NeuroImagen/lightning_logs/ContrastiveLossFeatureLearning/Adam_0.0001_LambdaLR_margin_1.5/weight-decay_0_lambda-factor_0.95/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | loss_fn | TripletMarginWithDistanceLoss | 0     \n",
      "1 | lstm    | LSTM                          | 132 K \n",
      "2 | output  | Sequential                    | 16.5 K\n",
      "----------------------------------------------------------\n",
      "148 K     Trainable params\n",
      "0         Non-trainable params\n",
      "148 K     Total params\n",
      "0.594     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▌         | 26/497 [00:02<00:39, 12.05it/s, v_num=0.95, train_loss_step=1.350, lr_step=8.57e-5, val_loss=1.500, train_loss_epoch=1.500, lr_epoch=9.02e-5] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "model = FeatureExtractor_ContrastiveLearning_NN()\n",
    "model.to(device)\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"/Users/ms/cs/ML/NeuroImagen/lightning_logs/ContrastiveLossFeatureLearning\",\n",
    "    name=f\"{config['optimizer']}_{config['lr']}_{config['scheduler']}_margin_{config['margin']}\",\n",
    "    version=f\"weight-decay_{config['weight-decay']}_lambda-factor_{config['lambda-factor']}\",\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "trainer = Trainer = L.Trainer(max_epochs=200, logger=logger, callbacks=[lr_monitor])\n",
    "trainer.fit(model, train_dataloaders=loaders[\"train\"], val_dataloaders=loaders[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"lightning_logs/ContrastiveLossFeatureLearning/Adam_1e-4_LambdaLR_0.95/version_8/checkpoints/epoch=15-step=7952.ckpt\"\n",
    "\n",
    "CKPT_PATH = os.path.join(root_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class EEG_Classifier(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.feature_extractor = FeatureExtractor_ContrastiveLearning_NN()\n",
    "        self.feature_extractor = (\n",
    "            FeatureExtractor_ContrastiveLearning_NN.load_from_checkpoint(CKPT_PATH)\n",
    "        )\n",
    "        self.feature_extractor.requires_grad_(False)\n",
    "        self.classifier = nn.Linear(128, 40)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # for p in self.feature_extractor.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        # with torch.no_grad():\n",
    "        #     input = self.feature_extractor(input)\n",
    "        input = self.feature_extractor(input)\n",
    "        res = self.classifier(input)\n",
    "        return res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        eegs, labels = batch\n",
    "        eegs = eegs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = self(eegs)\n",
    "\n",
    "        loss = self.loss_fn(out, labels)\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"lr\": self.scheduler.get_last_lr()[0]},\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_eval(batch, prefix=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._shared_eval(batch, prefix=\"test\")\n",
    "\n",
    "    def _shared_eval(self, batch, prefix=\"val\"):\n",
    "        eegs, labels = batch\n",
    "        out = self(eegs)\n",
    "        loss = self.loss_fn(out, labels)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        acc = accuracy(preds, labels, \"multiclass\", num_classes=preds.shape[0])\n",
    "        self.log_dict({f\"{prefix}_loss\": loss, f\"{prefix}_acc\": acc}, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, (lambda epoch: 0.90**epoch)\n",
    "        )\n",
    "        self.scheduler = scheduler\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/ms/cs/ML/NeuroImagen/lightning_logs/ContrastiveLossClassification/Adam_1e-3_LambdaLR_0.90\n",
      "\n",
      "  | Name              | Type                                    | Params\n",
      "------------------------------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractor_ContrastiveLearning_NN | 148 K \n",
      "1 | classifier        | Linear                                  | 5.2 K \n",
      "2 | loss_fn           | CrossEntropyLoss                        | 0     \n",
      "------------------------------------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "148 K     Non-trainable params\n",
      "153 K     Total params\n",
      "0.615     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  42%|████▏     | 207/497 [00:06<00:09, 29.84it/s, v_num=0, train_loss_step=3.720, lr_step=0.001]"
     ]
    }
   ],
   "source": [
    "model = EEG_Classifier()\n",
    "model.to(device)\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"/Users/ms/cs/ML/NeuroImagen/lightning_logs/ContrastiveLossClassification\",\n",
    "    name=\"Adam_1e-3_LambdaLR_0.90\",\n",
    "    version=None,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=200, logger=logger, callbacks=[lr_monitor])\n",
    "trainer.fit(model, train_dataloaders=loaders[\"train\"], val_dataloaders=loaders[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/ms/anaconda3/envs/neuroimagen/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 124/124 [00:04<00:00, 25.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.019657257944345474    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.7022695541381836     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.019657257944345474   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.7022695541381836    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 3.7022695541381836, 'test_acc': 0.019657257944345474}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dataloaders=loaders[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroimagen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
