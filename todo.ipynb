{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layout to image research (or depth-map to image?)\n",
    "  - use additional adapter?\n",
    "- use image embedding instead of caption embedding?\n",
    "- blip vs raw caption comparison (blip should be higher??)\n",
    "- understand diffusion model & clip & conditioning\n",
    "  - 77x768이 아니라 768로 해보기?\n",
    "- Layer modulation (SODA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layout to image (how?)\n",
    "  1. EEG -> text -> bbox regression\n",
    "  2. EEG -> MLP(bbox, concept)\n",
    "  3. Use MLLM (like UMBRAE)\n",
    "\n",
    "[] dataset frequency range 중요?\n",
    "[] 코드베이스 정리 (.env, configure github, 필요없는거 지우기)\n",
    "[] evaluation speed up?\n",
    "[] 논문 읽기\n",
    "  [] use transformer for encoder?\n",
    "  [] real time rendering?\n",
    "  [] CLIP text + image embedding? (gligen)\n",
    "  [] use location info in early sampling stage, original ldm in later stage.\n",
    "  [] ControlNet\n",
    "  [] T2I adapter color condition\n",
    "  [] style condition?\n",
    "  [] dataset split correct?\n",
    "  [] Alljoined dataset?\n",
    "  [] cascade diffusion network?\n",
    "\n",
    "Where I left off:\n",
    "[] VAE 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pooling 안되는듯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
